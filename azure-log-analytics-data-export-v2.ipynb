{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "357a3e78-a2de-4b57-9cbf-aca900106188",
   "metadata": {},
   "source": [
    "# Azure Log Analytics Data Export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620c7273-7c72-45f0-bf44-26ebf61ee944",
   "metadata": {},
   "source": [
    "This notebook demonstrates export of data (50M+ rows per day) from Azure Log Analytics to Blob Storage:\n",
    "\n",
    "Inputs and Outputs:\n",
    "- <b>Input:</b> table(s), columns, and date range\n",
    "- <b>Output:</b> json, csv, or parquet files\n",
    "\n",
    "Summary:\n",
    "1. <b>Generate Test Data:</b> ingests fake data for testing\n",
    "2. <b>Split Query and Send to Queue:</b> divides request into smaller queries/jobs and sends to a storage queue\n",
    "3. <b>Process Queue:</b> runs jobs from the storage queue and saves output to storage account"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1ba738-3b4c-4997-ba89-bb9ac878033f",
   "metadata": {},
   "source": [
    "# 1. Ingest Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119a4944-fc25-4ae5-8b1b-eda89bf008cc",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece3edce-6a60-4508-9cc5-8979c83a9bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.monitor.ingestion import LogsIngestionClient\n",
    "from azure.monitor.query import LogsQueryClient, LogsQueryStatus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51604221-8822-45ee-b2dd-ea72cebf44bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_timestamp = str(pd.Timestamp.today())\n",
    "logging_timestamp = (\n",
    "    logging_timestamp.replace(\"-\", \"\").replace(\":\", \"\").replace(\".\", \"\").replace(\" \", \"\")\n",
    ")\n",
    "logging.basicConfig(\n",
    "    filename=f\"log-analytics-ingest-{logging_timestamp}.log\",\n",
    "    format=\"%(asctime)s %(levelname)s %(message)s\",\n",
    "    filemode=\"w\",\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312222c9-891a-4327-8638-a8a721f59ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_log(input: str) -> None:\n",
    "    print(input)\n",
    "    logger.info(input)\n",
    "\n",
    "\n",
    "def query_log_qnalytics_request(\n",
    "    workspace_id: str, client: LogsQueryClient, kql_query: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Makes API query request to log analytics\n",
    "    limits: https://learn.microsoft.com/en-us/azure/azure-monitor/logs/api/timeouts\n",
    "    API query limits:\n",
    "        500,000 rows per request\n",
    "        200 requests per 30 seconds\n",
    "        max query time is 10 mins\n",
    "        100MB data max per request\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.query_workspace(\n",
    "            workspace_id=workspace_id,\n",
    "            query=kql_query,\n",
    "            timespan=None,\n",
    "            server_timeout=600,\n",
    "        )\n",
    "        if response.status == LogsQueryStatus.SUCCESS:\n",
    "            table = response.tables[0]\n",
    "            df = pd.DataFrame(data=table.rows, columns=table.columns)\n",
    "            return df\n",
    "        else:\n",
    "            raise Exception(f\"Unsucessful Request, Exception: {response.status}\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed Request, Exception: {response.status}\")\n",
    "\n",
    "\n",
    "def query_log_analytics_connection_request(\n",
    "    credential: DefaultAzureCredential, workspace_id: str, kql_query: str\n",
    ") -> pd.DataFrame:\n",
    "    # log analytics connection\n",
    "    # note: need to add Log Analytics Contributor role\n",
    "    # note: need to add Log Analytics Publisher role\n",
    "    log_client = LogsQueryClient(credential)\n",
    "    # submit query request\n",
    "    result_df = query_log_qnalytics_request(workspace_id, log_client, kql_query)\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def generate_fake_data_log_analtyics(\n",
    "    start_date: str,\n",
    "    timedelta_seconds: int,\n",
    "    number_of_rows: int,\n",
    "    number_of_columns: int,\n",
    "    random_length: int = 10,\n",
    ") -> pd.DataFrame:\n",
    "    # create dataframe\n",
    "    start_datetime = pd.to_datetime(start_date)\n",
    "    timedelta = pd.Series(range(number_of_rows)) * pd.to_timedelta(\n",
    "        f\"{timedelta_seconds}s\"\n",
    "    )\n",
    "    fake_time_column = start_datetime + timedelta\n",
    "    fake_data_df = pd.DataFrame(\n",
    "        {\n",
    "            \"TimeGenerated\": fake_time_column,\n",
    "        }\n",
    "    )\n",
    "    for each_index in range(1, number_of_columns):\n",
    "        each_column_name = f\"DataColumn{each_index}\"\n",
    "        each_column_value = \"\".join(\n",
    "            random.choice(string.ascii_lowercase) for i in range(random_length)\n",
    "        )\n",
    "        fake_data_df[each_column_name] = each_column_value\n",
    "    # convert datetime to string column to avoid issues in log analtyics\n",
    "    time_generated = fake_data_df[\"TimeGenerated\"].dt.strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    fake_data_df[\"TimeGenerated\"] = time_generated\n",
    "    # status\n",
    "    print(f\"Fake Data Shape: {fake_data_df.shape}\")\n",
    "    print(f\"Size: {fake_data_df.memory_usage().sum() / 1_000_000} MBs\")\n",
    "    print(f\"First Datetime: {fake_data_df['TimeGenerated'].iloc[0]}\")\n",
    "    print(f\"Last Datetime: {fake_data_df['TimeGenerated'].iloc[-1]}\")\n",
    "    return fake_data_df\n",
    "\n",
    "\n",
    "def log_analytics_ingest(\n",
    "    fake_data_df: pd.DataFrame,\n",
    "    ingest_client: LogsIngestionClient,\n",
    "    rule_id: str,\n",
    "    stream_name: str,\n",
    ") -> None:\n",
    "    # convert to json\n",
    "    body = json.loads(fake_data_df.to_json(orient=\"records\", date_format=\"iso\"))\n",
    "    # send to log analytics\n",
    "    ingest_client.upload(rule_id=rule_id, stream_name=stream_name, logs=body)\n",
    "\n",
    "\n",
    "def generate_and_ingest_fake_date(\n",
    "    credential: DefaultAzureCredential,\n",
    "    workspace_id: str,\n",
    "    endpoint: str,\n",
    "    rule_id: str,\n",
    "    stream_name: str,\n",
    "    start_date: str,\n",
    "    timedelta_seconds: int,\n",
    "    number_of_rows: int,\n",
    "    number_of_columns: int = 10,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates fake data and ingests in Log Analytics for testing\n",
    "        note: credential requires Log Analytics Contributor and Publisher roles\n",
    "        note: 10M rows with 10 columns takes about 15-20 minutes\n",
    "    Log Analytics Data Collection Endpoint and Rule setup:\n",
    "        1. azure portal -> monitor -> create data collection endpoint\n",
    "        2. azure protal -> log analtyics -> table -> create new custom table in log analtyics\n",
    "        3. create data collection rule and add publisher role permissions\n",
    "        reference: https://learn.microsoft.com/en-us/azure/azure-monitor/logs/tutorial-logs-ingestion-portal\n",
    "    Args:\n",
    "        credential: DefaultAzureCredential\n",
    "        workspace_id: log analytics workspace id\n",
    "            format: \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\"\n",
    "        endpoint: log analytics endpoint url\n",
    "            format: \"https://{name}-XXXX.eastus-1.ingest.monitor.azure.com\"\n",
    "        rule_id: required log analytics ingestion param\n",
    "            format: \"dcr-XXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
    "        stream_name: required log analytics ingestion param\n",
    "            format: \"Custom-{tablename}\"\n",
    "        start_date: date to insert fake data\n",
    "            format: \"02-08-2024 00:00:00.000000\"\n",
    "            note: can only ingest dates up to 2 days in the past and 1 day into the future\n",
    "            reference: https://learn.microsoft.com/en-us/azure/azure-monitor/logs/log-standard-columns\n",
    "        timedelta_seconds: time between each fake data row\n",
    "        number_of_rows: total number of rows to generate\n",
    "        number_of_columns: total number of columns to generate\n",
    "            note: for new columns, you need to update the schema before ingestion\n",
    "            1. azure portal -> log analytics -> settings - tables -> ... -> edit schema\n",
    "            2. azure portal -> data collection rules -> export template -> deploy -> edit\n",
    "    Returns:\n",
    "        pandas dataframe\n",
    "    \"\"\"\n",
    "    # input validation\n",
    "    given_timestamp = pd.to_datetime(start_date)\n",
    "    current_datetime = pd.to_datetime(\"today\")\n",
    "    check_start_range = current_datetime - pd.to_timedelta(\"2D\")\n",
    "    check_end_range = current_datetime + pd.to_timedelta(\"1D\")\n",
    "    if not (check_start_range <= given_timestamp <= check_end_range):\n",
    "        print_log(\"Warning: Date given is outside allowed ingestion range\")\n",
    "        print_log(\"Note: Log Analytics will use ingest time as TimeGenerated\")\n",
    "    if number_of_rows < 2 or number_of_columns < 2:\n",
    "        raise Exception(\"invalid row and/or column numbers\")\n",
    "    # log analytics ingest connection\n",
    "    ingest_client = LogsIngestionClient(endpoint, credential)\n",
    "    # genereate fake data\n",
    "    print_log(\"Generating Fake Data...\")\n",
    "    time_start = time.time()\n",
    "    fake_data_df = generate_fake_data_log_analtyics(\n",
    "        start_date, timedelta_seconds, number_of_rows, number_of_columns\n",
    "    )\n",
    "    print_log(\"Sending to Log Analytics...\")\n",
    "    # send to log analtyics\n",
    "    log_analytics_ingest(fake_data_df, ingest_client, rule_id, stream_name)\n",
    "    time_end = time.time()\n",
    "    print_log(f\"Runtime: {round(time_end-time_start,1)} seconds\")\n",
    "    return fake_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ca9cbf-8024-49ba-b502-711453a48ad6",
   "metadata": {},
   "source": [
    "## Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3474c1-8bff-4377-91b9-087f331ef1a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# auth\n",
    "# 1. service principal\n",
    "# os.environ[\"AZURE_CLIENT_ID\"] = \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\"\n",
    "# os.environ[\"AZURE_TENANT_ID\"] = \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\"\n",
    "# os.environ[\"AZURE_CLIENT_SECRET\"] = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
    "# 2. command line\n",
    "!az login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ccb098-f086-45dc-8b64-4c92fc9017ad",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398b493c-af04-4d03-9f35-e51ac26a9d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connection\n",
    "log_analytics_workspace_id = \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\"\n",
    "log_analytics_data_collection_endpoint = (\n",
    "    \"https://XXXXXXXXX-XXXXXX.eastus-1.ingest.monitor.azure.com\"\n",
    ")\n",
    "log_analytics_data_collection_rule_id = \"dcr-XXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
    "log_analytics_data_collection_stream_name = \"Custom-XXXXXXXXXXXXX_CL\"\n",
    "# params\n",
    "start_datetime = \"02-21-2024 00:00:00.000000\"\n",
    "time_delta_seconds = 0.0018\n",
    "number_of_rows = 50_000_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5272e9cb-d131-4f8b-a3fc-5cf1743487cb",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a367f3e2-12ff-4f1d-a244-3677869a0853",
   "metadata": {},
   "outputs": [],
   "source": [
    "credential = DefaultAzureCredential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3a3a88-b799-4df1-8928-c91dc5d00721",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fake_data_df = generate_and_ingest_fake_date(\n",
    "    credential,\n",
    "    log_analytics_workspace_id,\n",
    "    log_analytics_data_collection_endpoint,\n",
    "    log_analytics_data_collection_rule_id,\n",
    "    log_analytics_data_collection_stream_name,\n",
    "    start_datetime,\n",
    "    time_delta_seconds,\n",
    "    number_of_rows,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f17185-b1b7-4ffd-8c50-e5b5ec3edb72",
   "metadata": {},
   "source": [
    "## Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f215b1-ba42-4a5f-a3cb-3844eed28208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query log analtyics to confirm ingest\n",
    "log_analytics_table_name = log_analytics_data_collection_stream_name[7:]\n",
    "test_kql_query = f\"\"\"\n",
    "{log_analytics_table_name}\n",
    "| project-away TenantId, Type, _ResourceId\n",
    "| sort by TimeGenerated desc\n",
    "| take 100\n",
    "\"\"\"\n",
    "query_log_analytics_connection_request(\n",
    "    credential,\n",
    "    log_analytics_workspace_id,\n",
    "    test_kql_query,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d71ef7f-1efe-4909-9ced-252d62bca802",
   "metadata": {},
   "source": [
    "# 2. Split Query and Send to Queue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35693441-23c5-4418-aabb-da6cfb2956b7",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72e4922-152b-4f8b-8389-d1337c930224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.monitor.query import LogsQueryClient, LogsQueryStatus\n",
    "from azure.storage.queue import QueueClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e4ff3b-674d-4999-beee-37dfa0f4eebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_timestamp = str(pd.Timestamp.today())\n",
    "logging_timestamp = (\n",
    "    logging_timestamp.replace(\"-\", \"\").replace(\":\", \"\").replace(\".\", \"\").replace(\" \", \"\")\n",
    ")\n",
    "logging.basicConfig(\n",
    "    filename=f\"log-analytics-send-{logging_timestamp}.log\",\n",
    "    format=\"%(asctime)s %(levelname)s %(message)s\",\n",
    "    filemode=\"w\",\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910d7006-9ce5-4ccf-8b82-c66e335742fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_log(input: str) -> None:\n",
    "    print(input)\n",
    "    logger.info(input)\n",
    "\n",
    "\n",
    "def query_log_qnalytics_request(\n",
    "    workspace_id: str, client: LogsQueryClient, kql_query: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Makes API query request to log analytics\n",
    "    limits: https://learn.microsoft.com/en-us/azure/azure-monitor/logs/api/timeouts\n",
    "    API query limits:\n",
    "        500,000 rows per request\n",
    "        200 requests per 30 seconds\n",
    "        max query time is 10 mins\n",
    "        100MB data max per request\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.query_workspace(\n",
    "            workspace_id=workspace_id,\n",
    "            query=kql_query,\n",
    "            timespan=None,\n",
    "            server_timeout=600,\n",
    "        )\n",
    "        if response.status == LogsQueryStatus.SUCCESS:\n",
    "            table = response.tables[0]\n",
    "            df = pd.DataFrame(data=table.rows, columns=table.columns)\n",
    "            return df\n",
    "        else:\n",
    "            raise Exception(f\"Unsucessful Request, Exception: {response.status}\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed Request, Exception: {response.status}\")\n",
    "\n",
    "\n",
    "def query_log_analytics_get_table_columns(\n",
    "    table_names_and_columns: dict, workspace_id: str, client: LogsQueryClient\n",
    ") -> dict:\n",
    "    output = {}\n",
    "    for each_table, each_columns in table_names_and_columns.items():\n",
    "        # if column names provided, make no changes\n",
    "        if each_columns:\n",
    "            each_columns_fix = each_columns\n",
    "            if \"TimeGenerated\" not in each_columns:\n",
    "                each_columns_fix = [\"TimeGenerated\"] + each_columns\n",
    "            output[each_table] = each_columns_fix\n",
    "        # if no column names, query log analtyics for all column names\n",
    "        else:\n",
    "            # query log analtyics\n",
    "            try:\n",
    "                each_kql_query = f\"\"\"\n",
    "                let TABLE_NAME = \"{each_table}\";\n",
    "                table(TABLE_NAME)\n",
    "                | project-away TenantId, Type, _ResourceId\n",
    "                | take 1\n",
    "                \"\"\"\n",
    "                each_df = query_log_qnalytics_request(\n",
    "                    workspace_id, client, each_kql_query\n",
    "                )\n",
    "                each_columns_fix = list(each_df.columns)\n",
    "                each_columns_fix.remove(\"TimeGenerated\")\n",
    "                each_columns_fix = [\"TimeGenerated\"] + each_columns_fix\n",
    "                output[each_table] = each_columns_fix\n",
    "            # request unsucessful, most likely table not found\n",
    "            except Exception as e:\n",
    "                print_log(f\"{each_table}, Failed Request, Exception: {e}\")\n",
    "    if len(output) == 0:\n",
    "        raise Exception(\"No valid table names\")\n",
    "    return output\n",
    "\n",
    "\n",
    "def query_log_analytics_get_time_ranges(\n",
    "    workspace_id: str,\n",
    "    client: LogsQueryClient,\n",
    "    table_name: str,\n",
    "    start_datetime: str,\n",
    "    end_datetime: str,\n",
    "    query_row_limit: int,\n",
    ") -> pd.DataFrame:\n",
    "    # converted KQL output to string columns to avoid datetime digits getting truncated\n",
    "    kql_query = f\"\"\"\n",
    "    let TABLE_NAME = \"{table_name}\";\n",
    "    let START_DATETIME = datetime({start_datetime});\n",
    "    let END_DATETIME = datetime({end_datetime});\n",
    "    let QUERY_ROW_LIMIT = {query_row_limit};\n",
    "    let table_size = toscalar(table(TABLE_NAME) | count);\n",
    "    let time_splits = table(TABLE_NAME)\n",
    "    | project TimeGenerated\n",
    "    | where (TimeGenerated >= START_DATETIME) and (TimeGenerated < END_DATETIME)\n",
    "    | order by TimeGenerated asc\n",
    "    | extend row_index = row_number()\n",
    "    | where row_index == 1 or row_index % (QUERY_ROW_LIMIT) == 0 or row_index == table_size;\n",
    "    let time_pairs = time_splits\n",
    "    | project StartTime = TimeGenerated\n",
    "    | extend EndTime = next(StartTime)\n",
    "    | where isnotnull(EndTime)\n",
    "    | extend StartTime = tostring(StartTime), EndTime = tostring(EndTime);\n",
    "    time_pairs\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = query_log_qnalytics_request(workspace_id, client, kql_query)\n",
    "    # request unsucessful, most likely table not found\n",
    "    except Exception as e:\n",
    "        print_log(f\"{table_name}, Failed Request, Exception: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    # table exists, but no results returned\n",
    "    if df.shape[0] == 0:\n",
    "        return pd.DataFrame()\n",
    "    # datetime fix for events on final datetime\n",
    "    final_endtime = df[\"EndTime\"].iloc[-1]\n",
    "    new_final_endtime = str(pd.to_datetime(final_endtime) + pd.to_timedelta(\"0.0000001s\"))\n",
    "    new_final_endtime_fix_format = new_final_endtime.replace(\" \", \"T\").replace(\n",
    "        \"00+00:00\", \"Z\"\n",
    "    )\n",
    "    df[\"EndTime\"].iloc[-1] = new_final_endtime_fix_format\n",
    "    return df\n",
    "\n",
    "\n",
    "def query_log_analytics_get_table_count(\n",
    "    workspace_id: str,\n",
    "    client: LogsQueryClient,\n",
    "    table_name: str,\n",
    "    start_datetime: str,\n",
    "    end_datetime: str,\n",
    ") -> int:\n",
    "    kql_query = f\"\"\"\n",
    "    let TABLE_NAME = \"{table_name}\";\n",
    "    let START_DATETIME = datetime({start_datetime});\n",
    "    let END_DATETIME = datetime({end_datetime});\n",
    "    table(TABLE_NAME)\n",
    "    | project TimeGenerated\n",
    "    | where (TimeGenerated >= START_DATETIME) and (TimeGenerated < END_DATETIME)\n",
    "    | count\n",
    "    \"\"\"\n",
    "    df = query_log_qnalytics_request(workspace_id, client, kql_query)\n",
    "    return df.values[0][0]\n",
    "\n",
    "\n",
    "def query_log_analytics_add_table_row_counts(\n",
    "    input_df: pd.DataFrame,\n",
    "    workspace_id: str,\n",
    "    client: LogsQueryClient,\n",
    "    table_name: str,\n",
    ") -> pd.DataFrame:\n",
    "    # add row counts\n",
    "    results = []\n",
    "    for each_row in input_df.itertuples():\n",
    "        each_starttime = each_row.StartTime\n",
    "        each_endtime = each_row.EndTime\n",
    "        each_count = query_log_analytics_get_table_count(\n",
    "            workspace_id, client, table_name, each_starttime, each_endtime\n",
    "        )\n",
    "        results.append(each_count)\n",
    "    input_df[\"Count\"] = results\n",
    "    return input_df\n",
    "\n",
    "\n",
    "def query_log_analytics(\n",
    "    workspace_id: str,\n",
    "    client: LogsQueryClient,\n",
    "    table_name: str,\n",
    "    start_datetime: str,\n",
    "    end_datetime: str,\n",
    "    query_row_limit: int,\n",
    "    query_row_limit_correction: int,\n",
    "    add_row_counts: bool,\n",
    ") -> pd.DataFrame:\n",
    "    # fix for large number of events at same datetime\n",
    "    query_row_limit_fix = query_row_limit - query_row_limit_correction\n",
    "    # get time ranges\n",
    "    results_df = query_log_analytics_get_time_ranges(\n",
    "        workspace_id,\n",
    "        client,\n",
    "        table_name,\n",
    "        start_datetime,\n",
    "        end_datetime,\n",
    "        query_row_limit_fix,\n",
    "    )\n",
    "    # empty results\n",
    "    if results_df.shape[0] == 0:\n",
    "        return pd.DataFrame()\n",
    "    # add row counts column\n",
    "    if add_row_counts:\n",
    "        results_df = query_log_analytics_add_table_row_counts(\n",
    "            results_df, workspace_id, client, table_name\n",
    "        )\n",
    "        # warning if exceed query limit\n",
    "        if results_df.Count.gt(query_row_limit).any():\n",
    "            raise Exception(\"Sub-Query exceeds query row limit, change limit params\")\n",
    "    # add table name column\n",
    "    results_df.insert(loc=0, column=\"Table\", value=[table_name] * len(results_df))\n",
    "    return results_df\n",
    "\n",
    "\n",
    "def break_up_date_range_days(\n",
    "    table_name: str,\n",
    "    start_datetime: str,\n",
    "    end_datetime: str,\n",
    ") -> pd.DataFrame:\n",
    "    # break up date range\n",
    "    date_range = pd.date_range(start=start_datetime, end=end_datetime, freq=\"D\")\n",
    "    # convert timestamps back to strings\n",
    "    date_range = [str(each) for each in date_range.to_list()]\n",
    "    # add partial day (if needed)\n",
    "    if end_datetime[-8:] != \"00:00:00\":\n",
    "        date_range += [end_datetime]\n",
    "    # group into time pairs\n",
    "    time_pairs = [(date_range[i], date_range[i + 1]) for i in range(len(date_range) - 1)]\n",
    "    # convert to dataframe\n",
    "    df_time_pairs = pd.DataFrame(time_pairs, columns=[\"start_date\", \"end_date\"])\n",
    "    df_time_pairs.insert(loc=0, column=\"table\", value=[table_name] * len(df_time_pairs))\n",
    "    return df_time_pairs\n",
    "\n",
    "\n",
    "def break_up_query(\n",
    "    table_names: list[str],\n",
    "    start_datetime: str,\n",
    "    end_datetime: str,\n",
    ") -> pd.DataFrame:\n",
    "    results = []\n",
    "    # break up by table names\n",
    "    for each_table_name in table_names:\n",
    "        # break up date ranges by day\n",
    "        each_df = break_up_date_range_days(each_table_name, start_datetime, end_datetime)\n",
    "        results.append(each_df)\n",
    "    df_results = pd.concat(results)\n",
    "    return df_results\n",
    "\n",
    "\n",
    "def query_log_analytics_send_to_queue(\n",
    "    credential: DefaultAzureCredential,\n",
    "    subscription_id: str,\n",
    "    resource_group: str,\n",
    "    worksapce_name: str,\n",
    "    workspace_id: str,\n",
    "    storage_queue_url: str,\n",
    "    table_names_and_columns: dict,\n",
    "    start_datetime: str,\n",
    "    end_datetime: str,\n",
    "    query_row_limit: int = 500_000,\n",
    "    query_row_limit_correction: int = 1_000,\n",
    "    request_wait_seconds: int = 0.05,\n",
    "    add_row_counts: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generates\n",
    "        note: credential requires Log Analytics Contributor and Storage Queue Data Contributor roles\n",
    "        note: date range is processed as [start_datetime, end_datetime)\n",
    "    Args:\n",
    "        credential: azure default credential object\n",
    "        subscription_id: azure subscription id\n",
    "            format: \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\"\n",
    "        resource_group: azure resource group\n",
    "        workspace_name: name of log analytics workspace\n",
    "        workspace_id: log analtyics workspace id\n",
    "            format: \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\"\n",
    "        storage_queue_url: storage account queue url\n",
    "            format: \"https://{storage_account_name}.queue.core.windows.net/{queue_name}\"\n",
    "        table_names: dictionary of table names with columns to project\n",
    "            note: blank column list will detect and use all columns\n",
    "            format:  {\"table_name\" : [\"column_1\", \"column_2\", ... ], ... }\n",
    "        start_datetime: starting datetime, inclusive\n",
    "            format: YYYY-MM-DD HH:MM:SS\n",
    "        end_datetime: ending datetime, exclusive\n",
    "            format: YYYY-MM-DD HH:MM:SS\n",
    "        query_row_limit: max number of rows for each follow-up query/message\n",
    "        query_row_limit_correction: correction factor in case of overlaping data\n",
    "        request_wait_seconds: wait time between http requests\n",
    "        add_row_counts: adds expected row count for queries to messages\n",
    "    Return\n",
    "        None\n",
    "    \"\"\"\n",
    "    # input validation\n",
    "    try:\n",
    "        pd.to_datetime(start_datetime)\n",
    "        pd.to_datetime(end_datetime)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Invalid Datetime Format, Exception {e}\")\n",
    "    # log analytics connection\n",
    "    # note: need to add Log Analytics Contributor role\n",
    "    log_client = LogsQueryClient(credential)\n",
    "    # storage queue connection\n",
    "    # note: need to add Storage Queue Data Contributor role\n",
    "    queue_client = QueueClient.from_queue_url(storage_queue_url, credential)\n",
    "    # process table and column names\n",
    "    table_names_and_columns = query_log_analytics_get_table_columns(\n",
    "        table_names_and_columns, workspace_id, log_client\n",
    "    )\n",
    "    # break up queries by table and date ranges\n",
    "    table_names = list(table_names_and_columns.keys())\n",
    "    df_queries = break_up_query(table_names, start_datetime, end_datetime)\n",
    "    # query log analytis, gets datetime splits for query row limit\n",
    "    print_log(\"Querying Log Analytics...\")\n",
    "    query_results = []\n",
    "    for each_query in df_queries.itertuples():\n",
    "        each_table_name = each_query.table\n",
    "        each_start_datetime = each_query.start_date\n",
    "        each_end_datetime = each_query.end_date\n",
    "        each_results_df = query_log_analytics(\n",
    "            workspace_id,\n",
    "            log_client,\n",
    "            each_table_name,\n",
    "            each_start_datetime,\n",
    "            each_end_datetime,\n",
    "            query_row_limit,\n",
    "            query_row_limit_correction,\n",
    "            add_row_counts,\n",
    "        )\n",
    "        # check status\n",
    "        each_status = f\"{each_table_name}: \"\n",
    "        each_status += f\"{each_start_datetime} - {each_end_datetime} \"\n",
    "        each_status += f\"-> {each_results_df.shape[0]} Queries\"\n",
    "        print_log(each_status)\n",
    "        # skip empty results\n",
    "        if each_results_df.shape[0] > 0:\n",
    "            query_results.append(each_results_df)\n",
    "    if query_results:\n",
    "        # combine all results\n",
    "        results_df = pd.concat(query_results)\n",
    "        # add column names\n",
    "        column_names = results_df[\"Table\"].apply(lambda x: table_names_and_columns[x])\n",
    "        results_df.insert(loc=1, column=\"Columns\", value=column_names)\n",
    "        # add azure property columns\n",
    "        results_df.insert(loc=2, column=\"Subscription\", value=subscription_id)\n",
    "        results_df.insert(loc=3, column=\"ResourceGroup\", value=resource_group)\n",
    "        results_df.insert(loc=4, column=\"LogAnalyticsWorkspace\", value=worksapce_name)\n",
    "        results_df.insert(loc=5, column=\"LogAnalyticsWorkspaceId\", value=workspace_id)\n",
    "        # convert to dictionary\n",
    "        results = results_df.to_dict(orient=\"records\")\n",
    "        print_log(f\"Sending {len(results)} Query Messages to Storage Queue...\")\n",
    "        # send to queue\n",
    "        for each_job in results:\n",
    "            try:\n",
    "                each_job_json = json.dumps(each_job)\n",
    "                queue_client.send_message(each_job_json)\n",
    "            except Exception as e:\n",
    "                print_log(f\"Unable to submit query: {each_job}, exception: {e}\")\n",
    "            finally:\n",
    "                time.sleep(request_wait_seconds)\n",
    "        print_log(f\"Queue Status: {queue_client.get_queue_properties()}\")\n",
    "    # no results\n",
    "    else:\n",
    "        print_log(\"Error: No Query Messages Generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03533f2f-69e2-4c5c-a8c8-e42d50873504",
   "metadata": {},
   "source": [
    "## Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b8f904-22fe-44c6-aee2-007392334e14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# auth\n",
    "# 1. service principal\n",
    "# os.environ[\"AZURE_CLIENT_ID\"] = \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\"\n",
    "# os.environ[\"AZURE_TENANT_ID\"] = \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\"\n",
    "# os.environ[\"AZURE_CLIENT_SECRET\"] = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
    "# 2. command line\n",
    "!az login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a67a6a-5a57-4f1a-952f-86a45df6afd5",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930f8c3f-f9a8-454b-a73b-6de1e467d4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connections\n",
    "subscription_id = \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\"\n",
    "resource_group_name = \"XXXXXXXXXXXXXXXXXXXXX\"\n",
    "log_analytics_worksapce_name = \"XXXXXXXXXXX\"\n",
    "log_analytics_workspace_id = \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\"\n",
    "storage_queue_url = \"https://XXXXXXXXXXXXXXX.queue.core.windows.net/XXXXXXXXXXXXXX\"\n",
    "# params\n",
    "table_names_and_columns = {\n",
    "    \"XXXXXXXXXXX_CL\": [\n",
    "        \"TimeGenerated\",\n",
    "        \"DataColumn1\",\n",
    "        \"DataColumn2\",\n",
    "        \"DataColumn3\",\n",
    "        \"DataColumn4\",\n",
    "        \"DataColumn5\",\n",
    "        \"DataColumn6\",\n",
    "        \"DataColumn7\",\n",
    "        \"DataColumn8\",\n",
    "        \"DataColumn9\",\n",
    "    ]\n",
    "}\n",
    "start_datetime = \"2024-02-21 00:00:00\"\n",
    "end_datetime = \"2024-02-22 00:00:00\"\n",
    "query_row_limit = 500_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48de5a23-c068-4bdb-b88a-090d63729575",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a8fd84-62fa-4947-a8df-ca241d9ca1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "credential = DefaultAzureCredential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c014a2f-5daf-4dc4-a6cf-ef1430768323",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_log_analytics_send_to_queue(\n",
    "    credential,\n",
    "    subscription_id,\n",
    "    resource_group_name,\n",
    "    log_analytics_worksapce_name,\n",
    "    log_analytics_workspace_id,\n",
    "    storage_queue_url,\n",
    "    table_names_and_columns,\n",
    "    start_datetime,\n",
    "    end_datetime,\n",
    "    query_row_limit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23730de-8f9a-422f-9494-51019879dbd4",
   "metadata": {},
   "source": [
    "## Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be11e3da-192c-4db3-9749-6b49df42f9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview queue messages\n",
    "QueueClient.from_queue_url(storage_queue_url, credential).peek_messages(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9d0dec-584b-4f2a-970f-90c9dfef76df",
   "metadata": {},
   "source": [
    "# 3. Process Queue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4eb39a-eb84-4c83-83c6-20760ff13c21",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80952c8-64fa-4ab8-b5d4-643d0a82236c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from io import BytesIO, StringIO\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.monitor.query import LogsQueryClient, LogsQueryStatus\n",
    "from azure.storage.blob import ContainerClient\n",
    "from azure.storage.queue import QueueClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0073e2ed-9707-4a9f-b1ab-acc160f360a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_timestamp = str(pd.Timestamp.today())\n",
    "logging_timestamp = (\n",
    "    logging_timestamp.replace(\"-\", \"\").replace(\":\", \"\").replace(\".\", \"\").replace(\" \", \"\")\n",
    ")\n",
    "logging.basicConfig(\n",
    "    filename=f\"log-analytics-process-{logging_timestamp}.log\",\n",
    "    format=\"%(asctime)s %(levelname)s %(message)s\",\n",
    "    filemode=\"w\",\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400ab776-356b-48a1-b3a9-301ad6c0d30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_log(input: str) -> None:\n",
    "    print(input)\n",
    "    logger.info(input)\n",
    "\n",
    "\n",
    "def query_log_qnalytics_request(\n",
    "    workspace_id: str, client: LogsQueryClient, kql_query: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Makes API query request to log analytics\n",
    "    limits: https://learn.microsoft.com/en-us/azure/azure-monitor/logs/api/timeouts\n",
    "    API query limits:\n",
    "        500,000 rows per request\n",
    "        200 requests per 30 seconds\n",
    "        max query time is 10 mins\n",
    "        100MB data max per request\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.query_workspace(\n",
    "            workspace_id=workspace_id,\n",
    "            query=kql_query,\n",
    "            timespan=None,\n",
    "            server_timeout=600,\n",
    "        )\n",
    "        if response.status == LogsQueryStatus.SUCCESS:\n",
    "            table = response.tables[0]\n",
    "            df = pd.DataFrame(data=table.rows, columns=table.columns)\n",
    "            return df\n",
    "        else:\n",
    "            raise Exception(f\"Unsucessful Request, Exception: {response.status}\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed Request, Exception: {response.status}\")\n",
    "\n",
    "\n",
    "def query_log_analytics_get_results(\n",
    "    workspace_id: str,\n",
    "    client: LogsQueryClient,\n",
    "    table_name: str,\n",
    "    message_column_names: list[str],\n",
    "    start_datetime: str,\n",
    "    end_datetime: str,\n",
    ") -> pd.DataFrame:\n",
    "    columns_to_project = \", \".join(message_column_names)\n",
    "    kql_query = f\"\"\"\n",
    "    let TABLE_NAME = \"{table_name}\";\n",
    "    let START_DATETIME = datetime({start_datetime});\n",
    "    let END_DATETIME = datetime({end_datetime});\n",
    "    table(TABLE_NAME)\n",
    "    | project {columns_to_project}\n",
    "    | where (TimeGenerated >= START_DATETIME) and (TimeGenerated < END_DATETIME)\n",
    "    \"\"\"\n",
    "    df = query_log_qnalytics_request(workspace_id, client, kql_query)\n",
    "    return df\n",
    "\n",
    "\n",
    "def datetime_to_filename_safe(input: str) -> str:\n",
    "    # remove characters from timestamp to be filename safe/readable\n",
    "    output = input.replace(\"-\", \"\").replace(\":\", \"\").replace(\".\", \"\")\n",
    "    output = output.replace(\"T\", \"\").replace(\"Z\", \"\")\n",
    "    output = output.replace(\" \", \"\")\n",
    "    return output\n",
    "\n",
    "\n",
    "def download_blob(\n",
    "    filename: str,\n",
    "    credential: DefaultAzureCredential,\n",
    "    storage_blob_url_and_container: list[str],\n",
    ") -> pd.DataFrame:\n",
    "    # storage blob connection\n",
    "    # note: need to add Storage Blob Data Contributor role\n",
    "    storage_blob_url, storage_container_name = storage_blob_url_and_container\n",
    "    container_client = ContainerClient(\n",
    "        storage_blob_url, storage_container_name, credential\n",
    "    )\n",
    "    # download data\n",
    "    blob_client = container_client.get_blob_client(filename)\n",
    "    downloaded_blob = blob_client.download_blob()\n",
    "    if filename.endswith(\".json\"):\n",
    "        stream = StringIO(downloaded_blob.content_as_text())\n",
    "        output_df = pd.read_json(stream, lines=True)\n",
    "    elif filename.endswith(\".csv\"):\n",
    "        stream = StringIO(downloaded_blob.content_as_text())\n",
    "        output_df = pd.read_csv(stream)\n",
    "    elif filename.endswith(\".parquet\"):\n",
    "        stream = BytesIO()\n",
    "        downloaded_blob.readinto(stream)\n",
    "        output_df = pd.read_parquet(stream, engine=\"pyarrow\")\n",
    "    else:\n",
    "        raise Exception(\"file extension not supported\")\n",
    "    return output_df\n",
    "\n",
    "\n",
    "def list_blobs_df(\n",
    "    credential: DefaultAzureCredential, storage_blob_url_and_container: list[str]\n",
    ") -> pd.DataFrame:\n",
    "    # increase column display for longer filenames\n",
    "    pd.set_option(\"max_colwidth\", None)\n",
    "    # storage blob connection\n",
    "    # note: need to add Storage Blob Data Contributor role\n",
    "    storage_blob_url, storage_container_name = storage_blob_url_and_container\n",
    "    container_client = ContainerClient(\n",
    "        storage_blob_url, storage_container_name, credential\n",
    "    )\n",
    "    # get blobs\n",
    "    results = []\n",
    "    for each_file in container_client.list_blobs():\n",
    "        each_name = each_file.name\n",
    "        each_size_MB = each_file.size / 1_000_000\n",
    "        each_date = each_file.creation_time\n",
    "        results.append([each_name, each_size_MB, each_date])\n",
    "    # convert to dataframe\n",
    "    df = pd.DataFrame(results, columns=[\"filename\", \"file_size_mb\", \"creation_time\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_queue(\n",
    "    credential: DefaultAzureCredential,\n",
    "    storage_queue_url: str,\n",
    "    storage_blob_url_and_container: list[str],\n",
    "    output_format: str = \"JSONL\",\n",
    "    confirm_row_count: bool = True,\n",
    "    message_visibility_timeout_seconds: int = 1000,\n",
    "    azure_storage_connection_timeout_fix_seconds: int = 600,\n",
    "    request_wait_seconds: float = 0.05,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Processes Log Analytics query jobs/messages from a storage queue and exports to Blob Storage\n",
    "        note: credential requires Log Analytics Contributor, Storage Queue Data Contributor, and Stroage Blob Data Contributor roles\n",
    "        note: takes ~150 seconds for a query with 500k rows and 10 columns to csv (100 seconds for parquet)\n",
    "    Args:\n",
    "        credential: azure default credential object\n",
    "        storage_queue_url: storage account queue url\n",
    "            format: \"https://{storage_account_name}.queue.core.windows.net/{queue_name}\"\n",
    "        storage_blob_url_and_container: storage account blob url and container name\n",
    "            format: [\"https://{storage_account_name}.blob.core.windows.net/\", \"{container_name}\"]\n",
    "        output_format: output file format, options = \"JSONL\", \"CSV\", \"PARQUET\n",
    "            note: JSONL is json line delimited\n",
    "        confirm_row_count: enables check if row count in message matches downloaded data\n",
    "        message_visibility_timeout_seconds: number of seconds for queue message visibility\n",
    "        azure_storage_connection_timeout_fix_seconds: upload blob file timeout ins econds\n",
    "        request_wait_seconds: wait between http requests\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # input validation\n",
    "    support_file_formats = [\"JSONL\", \"CSV\", \"PARQUET\"]\n",
    "    if output_format not in support_file_formats:\n",
    "        raise Exception(\"File format not supported\")\n",
    "    # log analytics connection\n",
    "    # note: need to add Log Analytics Contributor role\n",
    "    log_client = LogsQueryClient(credential)\n",
    "    # storage queue connection\n",
    "    # note: need to add Storage Queue Data Contributor role\n",
    "    queue_client = QueueClient.from_queue_url(storage_queue_url, credential)\n",
    "    # storage blob connection\n",
    "    # note: need to add Storage Blob Data Contributor role\n",
    "    storage_blob_url, storage_container_name = storage_blob_url_and_container\n",
    "    container_client = ContainerClient(\n",
    "        storage_blob_url, storage_container_name, credential\n",
    "    )\n",
    "    # process messages from queue until empty\n",
    "    while True:\n",
    "        # get next message\n",
    "        try:\n",
    "            runtime_start = time.time()\n",
    "            queue_message = queue_client.receive_message(\n",
    "                visibility_timeout=message_visibility_timeout_seconds\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print_log(f\"Unable to get queue message, {e}\")\n",
    "            continue\n",
    "        finally:\n",
    "            time.sleep(request_wait_seconds)\n",
    "        if queue_message:\n",
    "            # extract message\n",
    "            print_log(f\"Processing Message: {queue_message.content}\")\n",
    "            try:\n",
    "                message_content = json.loads(queue_message.content)\n",
    "                message_table_name = message_content[\"Table\"]\n",
    "                message_column_names = message_content[\"Columns\"]\n",
    "                message_subscription = message_content[\"Subscription\"]\n",
    "                message_resource_group = message_content[\"ResourceGroup\"]\n",
    "                message_log_analytics_name = message_content[\"LogAnalyticsWorkspace\"]\n",
    "                message_log_analytics_id = message_content[\"LogAnalyticsWorkspaceId\"]\n",
    "                message_start_datetime = message_content[\"StartTime\"]\n",
    "                message_end_datetime = message_content[\"EndTime\"]\n",
    "                if confirm_row_count:\n",
    "                    message_result_count = message_content[\"Count\"]\n",
    "            except Exception as e:\n",
    "                print_log(f\"Unable to extract json, {queue_message}, {e}\")\n",
    "                continue\n",
    "            # query log analytics\n",
    "            try:\n",
    "                # get results\n",
    "                each_results_df = query_log_analytics_get_results(\n",
    "                    message_log_analytics_id,\n",
    "                    log_client,\n",
    "                    message_table_name,\n",
    "                    message_column_names,\n",
    "                    message_start_datetime,\n",
    "                    message_end_datetime,\n",
    "                )\n",
    "                # row count check\n",
    "                print_log(\n",
    "                    f\"Sucessfully Downloaded from Log Analytics: {each_results_df.shape}\"\n",
    "                )\n",
    "                if confirm_row_count:\n",
    "                    if each_results_df.shape[0] != message_result_count:\n",
    "                        print_log(f\"Row count doesn't match expected, {queue_message}\")\n",
    "                        continue\n",
    "            except Exception as e:\n",
    "                print_log(f\"Unable to query log analytics, {queue_message}, {e}\")\n",
    "                continue\n",
    "            finally:\n",
    "                time.sleep(request_wait_seconds)\n",
    "            # send to storage\n",
    "            try:\n",
    "                # datetime convversion (using pandas native export functions instead)\n",
    "                # dt.strftime('%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "                # extract datetime values for filename\n",
    "                first_timestamp = each_results_df[\"TimeGenerated\"].iloc[0]\n",
    "                extract_year = first_timestamp.strftime(\"%Y\")\n",
    "                extract_month = first_timestamp.strftime(\"%m\")\n",
    "                extract_day = first_timestamp.strftime(\"%d\")\n",
    "                extract_hour = first_timestamp.strftime(\"%H\")\n",
    "                # output filename\n",
    "                # mimics continuous export from log analytics\n",
    "                # https://learn.microsoft.com/en-us/azure/azure-monitor/logs/logs-data-export\n",
    "                each_filename = f\"{message_table_name}/\"\n",
    "                each_filename += f\"WorkspaceResourceId=/\"\n",
    "                each_filename += f\"subscriptions/{message_subscription}/\"\n",
    "                each_filename += f\"resourcegroups/{message_resource_group}/\"\n",
    "                each_filename += f\"providers/microsoft.operationalinsights/\"\n",
    "                each_filename += f\"workspaces/{message_log_analytics_name}/\"\n",
    "                each_filename += f\"y={extract_year}/m={extract_month}/d={extract_day}/\"\n",
    "                each_filename += f\"h={extract_hour}/\"\n",
    "                each_filename += f\"{datetime_to_filename_safe(message_start_datetime)}-\"\n",
    "                each_filename += f\"{datetime_to_filename_safe(message_end_datetime)}\"\n",
    "                # file format\n",
    "                if output_format == \"JSONL\":\n",
    "                    each_filename += \".json\"\n",
    "                    each_output_file = each_results_df.to_json(\n",
    "                        orient=\"records\", lines=True, date_format=\"iso\", date_unit=\"ns\"\n",
    "                    )\n",
    "                elif output_format == \"CSV\":\n",
    "                    each_filename += \".csv\"\n",
    "                    each_output_file = each_results_df.to_csv(index=False)\n",
    "                elif output_format == \"PARQUET\":\n",
    "                    each_filename += \".parquet\"\n",
    "                    each_output_file = each_results_df.to_parquet(\n",
    "                        index=False, engine=\"pyarrow\"\n",
    "                    )\n",
    "                # upload to storage\n",
    "                # note: set undocumented param connection_timeout to avoid timeout errors\n",
    "                # https://stackoverflow.com/questions/65092741/solve-timeout-errors-on-file-uploads-with-new-azure-storage-blob-package\n",
    "                blob_client = container_client.get_blob_client(each_filename)\n",
    "                blob_client_output = blob_client.upload_blob(\n",
    "                    data=each_output_file,\n",
    "                    connection_timeout=azure_storage_connection_timeout_fix_seconds,\n",
    "                    overwrite=True,\n",
    "                )\n",
    "                print_log(f\"Sucessfully Uploaded to Storage: {each_filename}\")\n",
    "            except Exception as e:\n",
    "                print_log(f\"Unable to upload to storage, {queue_message}, {e}\")\n",
    "                continue\n",
    "            finally:\n",
    "                time.sleep(request_wait_seconds)\n",
    "            # remove message from queue\n",
    "            try:\n",
    "                queue_client.delete_message(queue_message)\n",
    "                runtime_end = time.time()\n",
    "                print_log(f\"Sucessfully Deleted Message from Queue\")\n",
    "                print_log(f\"Runtime: {round(runtime_end-runtime_start, 1)} seconds\")\n",
    "            except Exception as e:\n",
    "                print_log(f\"Unable to delete message, {queue_message}, {e}\")\n",
    "                continue\n",
    "            finally:\n",
    "                time.sleep(request_wait_seconds)\n",
    "        # queue empty\n",
    "        else:\n",
    "            print_log(f\"Waiting for message visibility timeout...\")\n",
    "            time.sleep(message_visibility_timeout_seconds + 60)\n",
    "            # check if queue still empty\n",
    "            try:\n",
    "                peek_message = queue_client.peek_messages()\n",
    "                # exit program\n",
    "                if not peek_message:\n",
    "                    print_log(f\"Run Complete - No Messages to Process\")\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print_log(f\"Unable to peek queue message, {e}\")\n",
    "                continue\n",
    "            finally:\n",
    "                time.sleep(request_wait_seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b4d8f3-ce83-4142-95b8-63afc9ca7b1b",
   "metadata": {},
   "source": [
    "## Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b477f9b4-b679-4193-b1e1-d82c4a779057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auth\n",
    "# 1. service principal\n",
    "# os.environ[\"AZURE_CLIENT_ID\"] = \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\"\n",
    "# os.environ[\"AZURE_TENANT_ID\"] = \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\"\n",
    "# os.environ[\"AZURE_CLIENT_SECRET\"] = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n",
    "# 2. command line\n",
    "!az login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c309d590-7e38-43ec-b258-61d144dca816",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c005e22f-2107-4be1-96bb-5d0d611da181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connections\n",
    "storage_queue_url = \"https://XXXXXXXXXXXXXXXXXX.queue.core.windows.net/XXXXXXXXX\"\n",
    "storage_blob_url_and_container = [\n",
    "    \"https://XXXXXXXXXXXXXXXXX.blob.core.windows.net/\",\n",
    "    \"XXXXXXXXXX\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0aa1fb-cf64-4fe0-b653-c1fe2a9b6336",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674cc1f6-69ae-440b-99fa-404a62fd7554",
   "metadata": {},
   "outputs": [],
   "source": [
    "credential = DefaultAzureCredential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe640a60-d274-44df-b8fa-b6069274806d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "process_queue(credential, storage_queue_url, storage_blob_url_and_container)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae30247-1a7e-4adb-9fe9-527b2170cb1e",
   "metadata": {},
   "source": [
    "## Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d482edd5-cb5c-4cfe-8d9c-7f5527961a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list files\n",
    "list_blobs_df(credential, storage_blob_url_and_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1362b0ae-1ccc-4424-a135-4779ef786109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file\n",
    "file_list = list_blobs_df(credential, storage_blob_url_and_container)\n",
    "most_recent_filename = file_list.sort_values(\"creation_time\")[\"filename\"].iloc[0]\n",
    "download_blob(most_recent_filename, credential, storage_blob_url_and_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620758ef-1b3d-4980-8ca4-e37271d11606",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
